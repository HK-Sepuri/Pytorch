{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Test Train Split"
      ],
      "metadata": {
        "id": "NuLU_GYj1AIe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t52YPwZu0w7B"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming your data is in X (features) and y (target labels)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Explanation of arguments:\n",
        "# - X: Your feature data\n",
        "# - y: Your target labels\n",
        "# - test_size: Proportion of the data for the test set (default: 0.25)\n",
        "# - random_state: Seed for random number generation (ensures reproducibility)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Assuming your dataset is loaded as `dataset`\n",
        "train_size = int(0.8 * len(dataset))  # 80% for training\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Explanation:\n",
        "# - dataset: Your PyTorch dataset object\n",
        "# - [train_size, val_size]: List specifying lengths of each split\n"
      ],
      "metadata": {
        "id": "89Yuo2HL1Eew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Rate Schedulers in PyTorch"
      ],
      "metadata": {
        "id": "SoYifp8G1HTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReduceLROnPlateau:\n",
        "\n",
        "Reduces the learning rate when a monitored metric (e.g., validation loss) stops improving for a specified number of epochs (patience).\n",
        "Useful for preventing overfitting and helping the model converge."
      ],
      "metadata": {
        "id": "2Ge4rWb11ihD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "model = ...  # Your PyTorch model\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=3)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Train the model\n",
        "    ...\n",
        "    val_loss = compute_validation_loss(model, val_data)  # Replace with your validation logic\n",
        "    scheduler.step(val_loss)\n"
      ],
      "metadata": {
        "id": "qe9kIhhJ1fYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "StepLR:\n",
        "\n",
        "Decreases the learning rate by a multiplicative factor (gamma) every specified number of epochs (step_size).\n",
        "Simple and effective for gradually reducing the learning rate."
      ],
      "metadata": {
        "id": "23XhbBxx1m2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "model = ...  # Your PyTorch model\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Reduce lr by 10% every 10 epochs\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Train the model\n",
        "    ...\n",
        "    scheduler.step()  # Call after every epoch\n"
      ],
      "metadata": {
        "id": "76c_cq-U1lAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CosineAnnealingLR:\n",
        "\n",
        "Gradually reduces the learning rate using a cosine annealing schedule, starting high and decreasing to a minimum value over a specified number of epochs (T_max).\n",
        "Helps prevent overfitting and can lead to smoother convergence."
      ],
      "metadata": {
        "id": "UxjSNtST1qer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "model = ...  # Your PyTorch model\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=0.001)  # Reduce from 0.1 to 0.001 in 10 epochs\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Train the model\n",
        "    ...\n",
        "    scheduler.step()  # Call after every epoch\n"
      ],
      "metadata": {
        "id": "pMpziJ_21scV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improving model"
      ],
      "metadata": {
        "id": "ERULFm6g396k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Normalization (BatchNorm)\n",
        "\n",
        "Explanation:\n",
        "\n",
        "Normalizes the activations of a layer across a mini-batch, making the training process more stable and faster.\n",
        "Improves gradient flow by reducing internal covariate shift, allowing the optimizer to learn more efficiently.\n",
        "Often placed between convolutional layers and activation functions."
      ],
      "metadata": {
        "id": "jKJx0d8Q4Atl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)  # Example: Input channels=3, Output channels=6, Kernel size=5x5\n",
        "        self.bn1 = nn.BatchNorm2d(6)  # BatchNorm applied to 6 output channels\n",
        "        self.relu = nn.ReLU()\n",
        "        # ... other layers of your model\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)  # Normalize activations before ReLU\n",
        "        x = self.relu(x)\n",
        "        # ... forward pass through other layers\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "6GZE7wf24AGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout Layers\n",
        "\n",
        "Explanation:\n",
        "\n",
        "Randomly drops out a certain percentage of activations during training, preventing overfitting.\n",
        "Forces the network to learn more robust features that are not dependent on specific neurons.\n",
        "Typically placed after convolutional layers or fully connected layers."
      ],
      "metadata": {
        "id": "ttWB6ryL4FiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.2)  # Dropout with probability 0.2 (20%)\n",
        "        # ... other layers of your model\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)  # Apply dropout after ReLU\n",
        "        # ... forward pass through other layers\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "gC-9oNXk4Hxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization Techniques:\n",
        "\n",
        "Explanation:\n",
        "\n",
        "Penalize large weights in the network to prevent overfitting and encourage simpler representations.\n",
        "Common techniques include L1 (Lasso) and L2 (Ridge) regularization.\n",
        "Code (using L2 Regularization):"
      ],
      "metadata": {
        "id": "B5mj-iJN4Mf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim  # Import optim for optimizer\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        # ... other layers of your model\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        # ... forward pass through other layers\n",
        "        return x\n",
        "\n",
        "# Create the model and optimizer\n",
        "model = MyModel()\n",
        "optimizer = optim.Adam(model.parameters(), weight_decay=0.001)  # L2 regularization with weight_decay\n",
        "\n",
        "# Training loop (weight decay applied during backpropagation)\n",
        "# ...\n"
      ],
      "metadata": {
        "id": "FA9wcwpJ4J1-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}