{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Test Train Split"
      ],
      "metadata": {
        "id": "NuLU_GYj1AIe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t52YPwZu0w7B"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming your data is in X (features) and y (target labels)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Explanation of arguments:\n",
        "# - X: Your feature data\n",
        "# - y: Your target labels\n",
        "# - test_size: Proportion of the data for the test set (default: 0.25)\n",
        "# - random_state: Seed for random number generation (ensures reproducibility)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Assuming your dataset is loaded as `dataset`\n",
        "train_size = int(0.8 * len(dataset))  # 80% for training\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Explanation:\n",
        "# - dataset: Your PyTorch dataset object\n",
        "# - [train_size, val_size]: List specifying lengths of each split\n"
      ],
      "metadata": {
        "id": "89Yuo2HL1Eew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Rate Schedulers in PyTorch"
      ],
      "metadata": {
        "id": "SoYifp8G1HTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReduceLROnPlateau:\n",
        "\n",
        "Reduces the learning rate when a monitored metric (e.g., validation loss) stops improving for a specified number of epochs (patience).\n",
        "Useful for preventing overfitting and helping the model converge."
      ],
      "metadata": {
        "id": "2Ge4rWb11ihD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "model = ...  # Your PyTorch model\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=3)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Train the model\n",
        "    ...\n",
        "    val_loss = compute_validation_loss(model, val_data)  # Replace with your validation logic\n",
        "    scheduler.step(val_loss)\n"
      ],
      "metadata": {
        "id": "qe9kIhhJ1fYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "StepLR:\n",
        "\n",
        "Decreases the learning rate by a multiplicative factor (gamma) every specified number of epochs (step_size).\n",
        "Simple and effective for gradually reducing the learning rate."
      ],
      "metadata": {
        "id": "23XhbBxx1m2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "model = ...  # Your PyTorch model\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Reduce lr by 10% every 10 epochs\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Train the model\n",
        "    ...\n",
        "    scheduler.step()  # Call after every epoch\n"
      ],
      "metadata": {
        "id": "76c_cq-U1lAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CosineAnnealingLR:\n",
        "\n",
        "Gradually reduces the learning rate using a cosine annealing schedule, starting high and decreasing to a minimum value over a specified number of epochs (T_max).\n",
        "Helps prevent overfitting and can lead to smoother convergence."
      ],
      "metadata": {
        "id": "UxjSNtST1qer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "model = ...  # Your PyTorch model\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=0.001)  # Reduce from 0.1 to 0.001 in 10 epochs\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Train the model\n",
        "    ...\n",
        "    scheduler.step()  # Call after every epoch\n"
      ],
      "metadata": {
        "id": "pMpziJ_21scV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}